{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VP4IOS4o4Ogg",
    "tags": []
   },
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwG6c0fP2RMf",
    "outputId": "0e8336db-b40b-42b2-c4b2-81128f665e30"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miRsH6QQ4Scy"
   },
   "source": [
    "Function for loading the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "490C2OoPs4PX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def load_fashion_mnist():\n",
    "\n",
    "    url_base = \"https://www.math.unipd.it/~dasan/\"\n",
    "    Y_train = np.frombuffer(urlopen(url_base + \"train-labels-idx1-ubyte\").read(), dtype=np.uint8, offset=8)\n",
    "    X_train = np.frombuffer(urlopen(url_base + \"train-images-idx3-ubyte\").read(), dtype=np.uint8, offset=16).reshape(len(Y_train), 784) \n",
    "                                                                                            \n",
    "    Y_test = np.frombuffer(urlopen(url_base + \"t10k-labels-idx1-ubyte\").read(), dtype=np.uint8, offset=8)\n",
    "    X_test = np.frombuffer(urlopen(url_base + \"t10k-images-idx3-ubyte\").read(), dtype=np.uint8, offset=16).reshape(len(Y_test), 784)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "X_train_val, Y_train_val, X_test, Y_test = load_fashion_mnist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d14srm8Vh-b"
   },
   "source": [
    "First of all let's have a look of the data. The images are already in grey-scale since every pixel has just one number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "id": "CPzMfoAKxhxQ",
    "outputId": "caf31a5b-1897-435b-88fe-57a06d5a43f5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from collections import Counter\n",
    "\n",
    "print(X_train_val.shape, Y_train_val.shape, X_test.shape, Y_test.shape) #shape of the data\n",
    "#value of the data\n",
    "print(f\"Min: {X_train_val[0].min()}\")\n",
    "print(f\"Max: {X_train_val[0].max()}\")\n",
    "print(f\"Mean: {X_train_val[0].mean():.5f}\")\n",
    "print(f\"Min: {X_train_val[10].min()}\")\n",
    "print(f\"Max: {X_train_val[10].max()}\")\n",
    "print(f\"Mean: {X_train_val[10].mean():.5f}\")\n",
    "#grayscale goes from 0 to 255\n",
    "\n",
    "#what are our images\n",
    "labels_name={\n",
    "    0:'T-shirt/top',\n",
    "    1:'Trouser',\n",
    "    2:'Pullover',\n",
    "    3:'Dress',\n",
    "    4:'Coat',\n",
    "    5:'Sandal',\n",
    "    6:'Shirt',\n",
    "    7:'Sneaker',\n",
    "    8:'Bag',\n",
    "    9:'Ankle boot'\n",
    "}\n",
    "\n",
    "\n",
    "print(Counter(Y_train_val)) #classes are perfectly balanced\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(30, 9)) #data displayed\n",
    "for ax, x, label in zip(axes,X_train_val[0:10], Y_train_val[0:10]):\n",
    "  ax.set_axis_off()\n",
    "  X_image=x.reshape(28,28)\n",
    "  ax.imshow(X_image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "  ax.set_title(\"Training: %s\" % labels_name[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPTAfJLUmxdk"
   },
   "source": [
    "Data are perfectly balanced, so far as I know I won't need Under or Over Sampling methods.\n",
    "\n",
    "I build the validation and the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orWFcy3TnNVB",
    "outputId": "3e43041e-8754-47e6-eab9-acc584059464"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# we prefer a small validation set since dataset is quite big\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, Y_train_val, test_size=0.1, shuffle=False)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "print(Counter(y_train))\n",
    "#data are still balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xf7RK0271OBR"
   },
   "source": [
    "Now I build several type of training set and validation set:\n",
    "- two set preprocessed by MinMax scaler and Standard Scaler,\n",
    "- one set with a smaller size for classifiers that do not run in a reasonable time with the whole batch of data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fs0J3JDd1dPI"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#training and validation sets rescaled in 0-1 values \n",
    "minmax_scal=MinMaxScaler().fit(X_train_val)\n",
    "X_train_mm=minmax_scal.transform(X_train)\n",
    "X_val_mm=minmax_scal.transform(X_val)\n",
    "# this means X_train_mm=X_train/255\n",
    "\n",
    "#training and validation sets rescaled with 0 mean 1 std\n",
    "stand_scal=StandardScaler().fit(X_train_val)\n",
    "X_train_ss=stand_scal.transform(X_train)\n",
    "X_val_ss=stand_scal.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkYpGvNnxjm7",
    "outputId": "53d0457c-5108-4f6e-a1bb-c26fef1ca776"
   },
   "outputs": [],
   "source": [
    "#smaller data sets\n",
    "X_train_small, X_val_small, y_train_small, y_val_small = train_test_split(X_val, y_val, test_size=0.1, shuffle=False)\n",
    "print(X_train_small.shape, y_train_small.shape, X_val_small.shape, y_val_small.shape)\n",
    "print(Counter(y_train_small))\n",
    "\n",
    "#data more or less balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7mlYNbt5CNd"
   },
   "source": [
    "Let's start to implement the classifiers.\n",
    "\n",
    "For this classification task there's no reason to prefer a false positive to a false negative. We evaluate the performance of all the classifier by the accuracy on the validation set prediction.\n",
    "The best accuracy score on validation set for classifier is reported in the title of each paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6DExZfTLAE9",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 2) Decision Tree and Random Forest. (0.88250)\n",
    "\n",
    "I first run a Grid Search with a tree classifier trained on the small dataset.\n",
    "Than, knowing the best parameters, I fit a tree classifier on the whole batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9h8gbljZoz4",
    "outputId": "a1007904-c614-4bd1-cbbb-6b555d9b8081"
   },
   "outputs": [],
   "source": [
    "#runtime=8-9 minutes\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    " \n",
    "\n",
    "#parameters\n",
    "param_grid_test = {\n",
    "    'max_depth': [5,10,50,None],\n",
    "    'min_samples_split': [2,8,20],\n",
    "    'min_samples_leaf': [1,4,8]\n",
    "}\n",
    "\n",
    "#model definition\n",
    "model = tree.DecisionTreeClassifier(random_state=123,criterion='entropy')\n",
    "\n",
    "#grid-search object\n",
    "clf_2 = GridSearchCV(estimator = model, param_grid=param_grid_test, \n",
    "                   cv = 5, scoring = \"accuracy\",verbose=0)\n",
    "\n",
    "#training\n",
    "clf_2.fit(X_train_small,y_train_small)\n",
    "\n",
    "print(clf_2.best_params_)\n",
    "print(clf_2.best_score_)\n",
    "\n",
    "# expected output:\n",
    "# {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
    "# 0.757037037037037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dMSE9Tljrut",
    "outputId": "6a223da1-84a7-446b-9328-969836a402bb"
   },
   "outputs": [],
   "source": [
    "#new model\n",
    "clf_tree=tree.DecisionTreeClassifier(random_state=123,\n",
    "                                     criterion='entropy',\n",
    "                                     max_depth = 10,\n",
    "                                     min_samples_leaf = 4,\n",
    "                                     min_samples_split = 2 )\n",
    "\n",
    "#training on the whole batch of data\n",
    "clf_tree.fit(X_train,y_train)\n",
    "\n",
    "#make prediction\n",
    "y_pred_tree_train=clf_tree.predict(X_train) \n",
    "y_pred_tree_val=clf_tree.predict(X_val)\n",
    "print(f\"accuracy score trees train:\\t{accuracy_score(y_train,y_pred_tree_train):.5f}, \\tval: \\t{accuracy_score(y_val,y_pred_tree_val):.5f}\")\n",
    "\n",
    "#expected output:\n",
    "#accuracy score trees train:\t0.84770, \tval: \t0.80717"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5aujhpbNSQ8"
   },
   "source": [
    "I repeat the same process for a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Dp-hjHKgxf1",
    "outputId": "e5f30000-bfc7-49e1-f562-db02dde33b07"
   },
   "outputs": [],
   "source": [
    "#runtime= 10 min\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#parameters\n",
    "param_grid_test = {\n",
    "    'n_estimators':[5,10,100],\n",
    "    'max_depth': [5,10,None],\n",
    "    'min_samples_split': [2,10],\n",
    "    'min_samples_leaf': [1,10]\n",
    "}\n",
    "\n",
    "#model definition\n",
    "model = RandomForestClassifier(random_state=123,criterion='entropy')\n",
    "\n",
    "#grid-search object\n",
    "clf_3 = GridSearchCV(estimator = model, param_grid=param_grid_test, \n",
    "                   cv = 5, scoring = \"accuracy\",verbose=0)\n",
    "\n",
    "#training\n",
    "clf_3.fit(X_train_small,y_train_small)\n",
    "\n",
    "print(clf_3.best_params_)\n",
    "print(clf_3.best_score_)\n",
    "\n",
    "# expected output:\n",
    "# {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
    "# 0.8472222222222221\n",
    "# basically the default parameters ---> percfectly fit the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtzVDjQhmDEr",
    "outputId": "e0e71dad-40e6-4edf-84ee-ffd5a3b5b713"
   },
   "outputs": [],
   "source": [
    "#new model\n",
    "clf_forest=RandomForestClassifier(random_state=123,\n",
    "                                  criterion='entropy',\n",
    "                                  n_estimators = 100,\n",
    "                                  max_depth = None ,\n",
    "                                  min_samples_split =  2,\n",
    "                                  min_samples_leaf =  1\n",
    "                                  )\n",
    "\n",
    "#training\n",
    "clf_forest.fit(X_train,y_train)\n",
    "\n",
    "#make prediction\n",
    "y_pred_forest_train=clf_forest.predict(X_train)\n",
    "y_pred_forest_val=clf_forest.predict(X_val)\n",
    "print(f\"accuracy score trees train:\\t{accuracy_score(y_train,y_pred_forest_train):.5f}, \\tval: \\t{accuracy_score(y_val,y_pred_forest_val):.5f}\")\n",
    "\n",
    "#output\n",
    "#F1 score trees train:      \t1.00000, \tval: \t0.87996\n",
    "#accuracy score trees train:\t1.00000, \tval: \t0.88233"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNLBY6lqq1Ew"
   },
   "source": [
    "Let's have a look of the confusion matrices. Then I can visualize what tipe of samples are misclassified to understand how much big is the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "id": "rB9CEbnMqviP",
    "outputId": "c8d6cc11-b3e6-4b8d-ab66-dbd7b3f1fd9b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay \n",
    "\n",
    "cm_tree = confusion_matrix(y_val, y_pred_tree_val, labels=clf_2.classes_)\n",
    "print('       TREE: confusion matrix')\n",
    "print()\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_tree, display_labels=clf_tree.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cm_forest = confusion_matrix(y_val, y_pred_forest_val, labels=clf_forest.classes_)\n",
    "print('       FOREST: confusion matrix')\n",
    "print()\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_forest, display_labels=clf_forest.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EejNkQNyAfND"
   },
   "source": [
    "It seems that the target labeled with number 6 (Shirt) is often confused for class labeled with number 0 (T-shirt/top). \n",
    "If it was the only error it will be quite a good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "hix8UVqjrPG8",
    "outputId": "feaf6f63-5bf8-4d9d-d5a3-5d5d29c38e63"
   },
   "outputs": [],
   "source": [
    "indices = np.all([ (y_val[0:600]==6), (y_pred_forest_val[0:600]==0) ], axis=0) \n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=sum(indices==True), figsize=(20, 6))\n",
    "for ax, x, label, pred_label in zip(axes, X_val[0:600][indices], y_val[0:600][indices], y_pred_forest_val[0:600][indices]):\n",
    "  ax.set_axis_off()\n",
    "  ax.imshow(x.reshape(28,28), cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "  ax.set_title(\"Test: %s\\nPred: %s\" % (labels_name[label], labels_name[pred_label]))\n",
    "\n",
    "#scambia facilmente i vestitini per magliette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xm93nI0cRite"
   },
   "source": [
    "## What happen with scaled data?\n",
    "\n",
    "The best accuracy score on validation set is reached using minmax scaler and a random forest classifier.\n",
    "Actually there are not real improvements in using Standard scaled datasets or hog preprocessed data, uncomment other lines to see what happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5KBAul0jPYV-",
    "outputId": "3568d3c1-0f41-49e6-d934-4ccc7ca0f2d0"
   },
   "outputs": [],
   "source": [
    "#minmax scaler\n",
    "\n",
    "\"\"\"\n",
    "#tree\n",
    "\n",
    "clf_2_mm=tree.DecisionTreeClassifier(random_state=123,\n",
    "                                     criterion='entropy',\n",
    "                                     max_depth = 10,\n",
    "                                     min_samples_leaf = 4,\n",
    "                                     min_samples_split = 2 ) \n",
    "\n",
    "#training\n",
    "clf_2_mm.fit(X_train_mm,y_train)\n",
    "\n",
    "#prediction\n",
    "y_pred_tree_train_mm=clf_2_mm.predict(X_train_mm)\n",
    "y_pred_tree_val_mm=clf_2_mm.predict(X_val_mm)\n",
    "print(f\"Decision Tree with minmax scaler data\")\n",
    "print(f\"accuracy score trees train:\\t{accuracy_score(y_train,y_pred_tree_train_mm):.5f}, \\tval: \\t{accuracy_score(y_val,y_pred_tree_val_mm):.5f}\")\n",
    "\"\"\"\n",
    "\n",
    "#forest\n",
    "clf_3_mm=RandomForestClassifier(random_state=123,\n",
    "                                  criterion='entropy',\n",
    "                                  n_estimators = 100,\n",
    "                                  max_depth = None ,\n",
    "                                  min_samples_split =  2,\n",
    "                                  min_samples_leaf =  1\n",
    "                                  )\n",
    "\n",
    "#training\n",
    "clf_3_mm.fit(X_train_mm,y_train)\n",
    "\n",
    "#prediction\n",
    "y_pred_forest_train_mm=clf_3_mm.predict(X_train_mm)\n",
    "y_pred_forest_val_mm=clf_3_mm.predict(X_val_mm)\n",
    "print(f\"Random Forest with minmax scaler data\")\n",
    "print(f\"accuracy score trees train:\\t{accuracy_score(y_train,y_pred_forest_train_mm):.5f}, \\tval: \\t{accuracy_score(y_val,y_pred_forest_val_mm):.5f}\")\n",
    "\n",
    "#expected output:\n",
    "#Decision Tree with minmax scaler data\n",
    "#accuracy score trees train:\t0.84770, \tval: \t0.80717\n",
    "#Random Forest with minmax scaler data\n",
    "#accuracy score trees train:\t1.00000, \tval: \t0.88250# best for forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "HmkYP6jaRmzA",
    "outputId": "19a6c71d-e7b0-48fa-ed3d-9faed68168eb"
   },
   "outputs": [],
   "source": [
    "#standard scaler\n",
    "\n",
    "\"\"\"\n",
    "#tree\n",
    "clf_2_ss=tree.DecisionTreeClassifier(random_state=123,\n",
    "                                     criterion='entropy',\n",
    "                                     max_depth = 10,\n",
    "                                     min_samples_leaf = 4,\n",
    "                                     min_samples_split = 2 ) \n",
    "\n",
    "#training\n",
    "clf_2_ss.fit(X_train_ss,y_train)\n",
    "\n",
    "#prediction\n",
    "y_pred_tree_train_ss=clf_2_ss.predict(X_train_ss) \n",
    "y_pred_tree_val_ss=clf_2_ss.predict(X_val_ss)\n",
    "print(f\"Decision Tree with Standard scaler data\")\n",
    "print(f\"accuracy score trees train:\\t{accuracy_score(y_train,y_pred_tree_train_ss):.5f}, \\tval: \\t{accuracy_score(y_val,y_pred_tree_val_ss):.5f}\")\n",
    "\n",
    "\n",
    "#forest\n",
    "clf_3_ss=RandomForestClassifier(random_state=123,\n",
    "                                  criterion='entropy',\n",
    "                                  n_estimators = 100,\n",
    "                                  max_depth = None ,\n",
    "                                  min_samples_split =  2,\n",
    "                                  min_samples_leaf =  1\n",
    "                                  )\n",
    "\n",
    "#training\n",
    "clf_3_ss.fit(X_train_ss,y_train)\n",
    "\n",
    "#prediction\n",
    "y_pred_forest_train_ss=clf_3_ss.predict(X_train_ss)\n",
    "y_pred_forest_val_ss=clf_3_ss.predict(X_val_ss)\n",
    "print(f\"Random Forest with Standard scaler data\")\n",
    "print(f\"accuracy score trees train:\\t{accuracy_score(y_train,y_pred_forest_train_ss):.5f}, \\tval: \\t{accuracy_score(y_val,y_pred_forest_val_ss):.5f}\")\n",
    "\"\"\"\n",
    "\n",
    "#expected output:\n",
    "#Decision Tree with Standard scaler data\n",
    "#accuracy score trees train:\t0.84770, \tval: \t0.80700\n",
    "#Random Forest with Standard scaler data\n",
    "#accuracy score trees train:\t1.00000, \tval: \t0.88233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "JIa4bwsAPd_p",
    "outputId": "82d25de1-d2bf-4c73-d99b-afdc65d6fc3f"
   },
   "outputs": [],
   "source": [
    "#hog transformation\n",
    "\n",
    "\"\"\"\n",
    "#tree\n",
    "clf_2_hog=tree.DecisionTreeClassifier(random_state=123,\n",
    "                                     criterion='entropy',\n",
    "                                     max_depth = 10,\n",
    "                                     min_samples_leaf = 4,\n",
    "                                     min_samples_split = 2 ) \n",
    "\n",
    "#training\n",
    "clf_2_hog.fit(X_train_hog,y_train_small)\n",
    "\n",
    "#prediction\n",
    "y_pred_tree_train_hog=clf_2_hog.predict(X_train_hog) \n",
    "y_pred_tree_val_hog=clf_2_hog.predict(X_val_hog)\n",
    "print(f\"Decision Tree with hog transformed data\")\n",
    "print(f\"accuracy score trees train:\\t{accuracy_score(y_train_small,y_pred_tree_train_hog):.5f}, \\tval: \\t{accuracy_score(y_val_small,y_pred_tree_val_hog):.5f}\")\n",
    "\n",
    "\n",
    "#forest\n",
    "clf_3_hog=RandomForestClassifier(random_state=123,\n",
    "                                  criterion='entropy',\n",
    "                                  n_estimators = 100,\n",
    "                                  max_depth = None ,\n",
    "                                  min_samples_split =  2,\n",
    "                                  min_samples_leaf =  1\n",
    "                                  )\n",
    "\n",
    "#training\n",
    "clf_3_hog.fit(X_train_hog,y_train_small)\n",
    "\n",
    "#prediction\n",
    "y_pred_forest_train_hog=clf_3_hog.predict(X_train_small)\n",
    "y_pred_forest_val_hog=clf_3_hog.predict(X_val_small)\n",
    "print(f\"Random Forest with hog transformed data\")\n",
    "print(f\"accuracy score trees train:\\t{accuracy_score(y_train_small,y_pred_forest_train_hog):.5f}, \\tval: \\t{accuracy_score(y_val_small,y_pred_forest_val_hog):.5f}\")\n",
    "\"\"\"\n",
    "\n",
    "#expected output:\n",
    "#Decision Tree with hog transformed data\n",
    "#accuracy score trees train:\t0.90815, \tval: \t0.77167\n",
    "#Random Forest with hog transformed data\n",
    "#accuracy score trees train:\t0.36852, \tval: \t0.37500\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WBI374lu333",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 3) Support Vector Machine. (0.89350)\n",
    "\n",
    "SVM can run only with small data set, training on the all data takes 3 minutes and make a prediction on the training set takes 14 minutes. For this reason I prefer to avoid to make a prediction for the training set, computing the accuracy score only for the validation set. (Uncomment the cell of the training set to see its accuracy score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6UGSqg5vDLe",
    "outputId": "1e5606aa-dd83-4e9d-f8f6-8e260bae1ff3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "#parameters\n",
    "svc_grid_params = {\n",
    "    'C': ( 1, 10, 100),\n",
    "    'kernel': ('rbf', 'linear'),\n",
    "}\n",
    "\n",
    "#grid search object\n",
    "model = svm.SVC(random_state = 123)\n",
    "clf_4 = GridSearchCV(estimator = model,\n",
    "                     param_grid = svc_grid_params, \n",
    "                     scoring='accuracy',\n",
    "                     n_jobs= -1, cv = 5, verbose = 1)\n",
    "\n",
    "#training \n",
    "clf_4.fit(X_train_small,y_train_small)\n",
    "\n",
    "\n",
    "print(clf_4.best_params_)\n",
    "print(clf_4.best_score_)\n",
    "\n",
    "#expected output:\n",
    "#Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
    "#{'C': 100, 'kernel': 'rbf'}\n",
    "#0.8585185185185186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9M_CvSMvOto",
    "outputId": "1cfe97e8-acd4-4fbf-8301-5d12175a3eb0"
   },
   "outputs": [],
   "source": [
    "#model definition with best parameters\n",
    "clf_svm=svm.SVC(random_state = 123,\n",
    "                kernel = 'rbf',\n",
    "                C=100)\n",
    "\n",
    "#training \n",
    "clf_svm.fit(X_train,y_train)\n",
    "\n",
    "#make a prediction for the validation set\n",
    "y_pred_svm_val=clf_svm.predict(X_val) \n",
    "print(f\"accuracy score SVM on validation set: \\t{accuracy_score(y_val,y_pred_svm_val):.5f}\")\n",
    "\n",
    "#expected output:\n",
    "#accuracy score SVM on validation set: \t0.89183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "9HD9YD5x7JO7",
    "outputId": "2419d347-f45f-4f78-e193-2ca7c25215b2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "y_pred_svm_train=clf_svm.predict(X_train) \n",
    "print(f\"accuracy score SVM on trainin set: \\t{accuracy_score(y_train,y_pred_svm_train):.5f}\")\n",
    "\"\"\"\n",
    "\n",
    "#expected output\n",
    "#accuracy score SVM on trainin set: \t0.99969"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-5q5BrafMxw"
   },
   "source": [
    "## What happen with scaled data?\n",
    "\n",
    "The best accuracy score is reached using the Standard Scaled data and is not so better than the accuracy score just gained. \n",
    "Uncomment the other lines to see what happen with MinMax scaler and hog transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "THM2Gj8ygOAQ",
    "outputId": "8097e29c-bd68-4116-ee34-947a2e6fff73"
   },
   "outputs": [],
   "source": [
    "#minmax scaler\n",
    "\n",
    "\"\"\"\n",
    "#model\n",
    "clf_svm_mm=svm.SVC(random_state = 123,\n",
    "                kernel = 'rbf',\n",
    "                C=100)\n",
    "\n",
    "#training \n",
    "clf_svm_mm.fit(X_train_mm,y_train)\n",
    "\n",
    "#make a prediction for the validation set\n",
    "y_pred_svm_val=clf_svm_mm.predict(X_val_mm) \n",
    "print(f\"accuracy score SVM  with MinMax scaler data on val: \\t{accuracy_score(y_val,y_pred_svm_val):.5f}\")\n",
    "\"\"\"\n",
    "\n",
    "#Standard scaler\n",
    "\n",
    "#model\n",
    "clf_svm_ss=svm.SVC(random_state = 123,\n",
    "                kernel = 'rbf',\n",
    "                C=100)\n",
    "\n",
    "#training \n",
    "clf_svm_ss.fit(X_train_ss,y_train)\n",
    "\n",
    "#make a prediction for the validation set\n",
    "y_pred_svm_val=clf_svm_ss.predict(X_val_ss) \n",
    "print(f\"accuracy score SVM  with Standardscaler data on val: \\t{accuracy_score(y_val,y_pred_svm_val):.5f}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#Hog transformed data\n",
    "\n",
    "#model\n",
    "clf_svm_hog=svm.SVC(random_state = 123,\n",
    "                kernel = 'rbf',\n",
    "                C=100)\n",
    "\n",
    "#training \n",
    "clf_svm_hog.fit(X_train_hog,y_train_small)\n",
    "\n",
    "#We can make prediction on both train and val because we are using small data set\n",
    "y_pred_svm_train=clf_svm_hog.predict(X_train_hog)\n",
    "y_pred_svm_val=clf_svm_hog.predict(X_val_hog) \n",
    "print(f\"accuracy score SVM  with Hog transformed data on train: \\t{accuracy_score(y_train_small,y_pred_svm_train):.5f}, val: \\t{accuracy_score(y_val_small,y_pred_svm_val):.5f}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#questi score sono falsati perchè il validation è calcolato su un set che \n",
    "#expected output:\n",
    "#accuracy score SVM  with MinMax scaler data on val: \t0.89167\n",
    "#accuracy score SVM  with Standardscaler data on val: \t0.89350\n",
    "#accuracy score SVM  with Hog trnasformed data on train: \t1.00000, val: \t0.84333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Dnzhz-uyto7",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 4) Artificial Neural Network. (0.89417)\n",
    "\n",
    "In Keras documentation we can read that \"images need to be read and decoded into integer tensors, then converted to floating point and normalized to small values (usually between 0 and 1)\".\n",
    "\n",
    "For this reason in the ANN implementation I use only data preprocessed by MinMax scaler and Standard Scaler.\n",
    "\n",
    "# First of all I implement a function that allows me to make e grid search for the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThIn_s6FjFn1"
   },
   "outputs": [],
   "source": [
    "#libraries I need\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import tensorflow \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Input, Dense \n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.python.framework.random_seed import set_random_seed\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pj247o8AibM5"
   },
   "outputs": [],
   "source": [
    "#function for define and train more NN\n",
    "def MLP(X,y,vector_units,n_epochs,stop,n_patience):\n",
    "    #X already scaled\n",
    "    #y not transformed by to_categorical\n",
    "    \n",
    "    #processing data\n",
    "    features=np.shape(X)[1] #the input size\n",
    "    num_classes=len(set(y))\n",
    "    np.random.seed(123)\n",
    "    set_random_seed(2)\n",
    "\n",
    "    model=Sequential()\n",
    "    n_layers=len(vector_units) #we consider only the hidden layer\n",
    "    if n_layers==0:\n",
    "      model.add(Dense(input_dim=features, \n",
    "                      units = num_classes, \n",
    "                      activation= 'softmax'))\n",
    "    else:\n",
    "      for i in range(n_layers):\n",
    "        if i==0:\n",
    "          model.add(Dense(input_dim=features,\n",
    "                          units=vector_units[0],\n",
    "                          activation='relu'))\n",
    "        else: \n",
    "          model.add(Dense(units=vector_units[i],\n",
    "                          activation='relu'))\n",
    "      model.add(Dense(units = num_classes, \n",
    "                      activation= 'softmax'))\n",
    "    \n",
    "    #configure the model\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                optimizer='sgd', \n",
    "                metrics=['accuracy']) \n",
    "    \n",
    "    #EarlyStopping\n",
    "    if stop=='True':\n",
    "      stop_er=EarlyStopping(monitor='val_loss', #quantity to be monitored\n",
    "                    mode='min', #we look for decreasing patterns stop \n",
    "                    patience = n_patience, #number of epochs with no improvements\n",
    "                    verbose=1)\n",
    "    else:\n",
    "      stop_er=None\n",
    "    #training\n",
    "    y=to_categorical(y,num_classes)\n",
    "    train=model.fit(X,y,epochs=n_epochs,\n",
    "                    batch_size=32, \n",
    "                    verbose=0,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=stop_er)\n",
    "    \n",
    "    return model,train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9rfA3d4PDke"
   },
   "source": [
    "I create two smaller data sets of scaled data and the one-hot encoding of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5noochHlvzu"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#smaller data set for the grid search\n",
    "minmax_scal=MinMaxScaler().fit(X_train_small)\n",
    "X_train_mms=minmax_scal.transform(X_train_small) \n",
    "X_val_mms=minmax_scal.transform(X_val_small) \n",
    "\n",
    "#second smaller dataset for a comparison\n",
    "standard_scal=StandardScaler().fit(X_train_small)\n",
    "X_train_sss=standard_scal.transform(X_train_small) \n",
    "X_val_sss=standard_scal.transform(X_val_small) \n",
    "\n",
    "#one hot encoding\n",
    "y_train_cs=to_categorical(y_train_small,10)\n",
    "y_val_cs=to_categorical(y_val_small,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87faU3Ucjzrf"
   },
   "source": [
    "Now I can train several NN to find the best parameters.\n",
    "Even though I used smaller data set this cell requires 15 or more minutes to run. Since this is actually the best model among the others it's worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IVcs72Yry8PH",
    "outputId": "d1d7604a-4768-4713-8b31-a6dd7220d107"
   },
   "outputs": [],
   "source": [
    "#parameters of the function\n",
    "\n",
    "#to avoid a 50 minutes run I put a shortes version of vector_units_list but the results is the same\n",
    "# vector_units_list=[[784],[784,10],[500,200],[784,100,50],[784,200,50,10]] #long research\n",
    "vector_units_list=[[784],[784,10]]\n",
    "n_epochs_list=[100,500]\n",
    "n_patience_list=[5,15,25]\n",
    "stop='True'\n",
    "#let save the maximum accuracy\n",
    "acc=0\n",
    "accuracy_test=[]\n",
    "i=0\n",
    "#grid-search\n",
    "for n_epochs in n_epochs_list:\n",
    "  for vector in vector_units_list:\n",
    "    for n_patience in n_patience_list:\n",
    "\n",
    "      #train a NN \n",
    "      m,t=MLP(X_train_mms,y_train_small,vector,n_epochs,stop,n_patience)\n",
    "      print(i)\n",
    "      i+=1\n",
    "      #save the greater accuracy\n",
    "      accuracy_val=m.evaluate(X_val_mms,y_val_cs,verbose=0)[1]\n",
    "      if accuracy_val>acc:\n",
    "        acc=accuracy_val\n",
    "        win=[vector,n_epochs,n_patience]\n",
    "        model=m\n",
    "        accuracy_test.append(accuracy_val)\n",
    "\n",
    "print('\\n\\n\\n')\n",
    "print(model.summary())\n",
    "print('best parameters: ',win)\n",
    "plt.plot(accuracy_test)\n",
    "plt.title(f\"accuracy growing\")\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('grid search')\n",
    "plt.legend(['test acc'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#expected output:\n",
    "# best parameters: [[784], 100, 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK_fZLjvk4DI"
   },
   "source": [
    "Now I can train on the whole batch of data one Artificial Neural Network with the best parameters found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7PNQAczzMcg",
    "outputId": "e85aa369-0359-4fad-b4f1-c3f6843ede99"
   },
   "outputs": [],
   "source": [
    "#runtime= ~10 minutes\n",
    "\n",
    "#early stopping\n",
    "stop=EarlyStopping(monitor='val_loss', #quantity to be monitored\n",
    "                    mode='min', #we look for decreasing patterns stop \n",
    "                    patience = 15, #number of epochs with no improvements\n",
    "                    verbose = 1)\n",
    "#data\n",
    "features=np.shape(X_train)[1]\n",
    "num_classes=10 \n",
    "y_train_cat=to_categorical(y_train,num_classes)\n",
    "y_val_cat=to_categorical(y_val,num_classes)\n",
    "\n",
    "\n",
    "#model defining\n",
    "np.random.seed(123)\n",
    "set_random_seed=2\n",
    "\n",
    "nn=Sequential()\n",
    "nn.add(Dense(input_dim=features,\n",
    "             units=features,\n",
    "             activation='relu'))\n",
    "nn.add(Dense(num_classes,\n",
    "             activation='softmax'))\n",
    "print(nn.summary())\n",
    "\n",
    "#compiling and training\n",
    "nn.compile(loss='categorical_crossentropy',\n",
    "           optimizer='sgd',\n",
    "           metrics=['accuracy'])\n",
    "train=nn.fit(X_train_mm,y_train_cat,\n",
    "             batch_size=32,\n",
    "             verbose=0,\n",
    "             callbacks=stop,\n",
    "             epochs=100,\n",
    "             validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "Ptn1BME5zteU",
    "outputId": "e9ad15ee-b209-44ae-e057-49d14839e375"
   },
   "outputs": [],
   "source": [
    "#display the accuracy \n",
    "plt.plot(train.history['accuracy'])\n",
    "plt.plot(train.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','validation split'],loc='down right',fontsize='x-large')\n",
    "plt.show()\n",
    "\n",
    "#make prediction on training and validation set\n",
    "y_pred_nn_train=nn.predict(X_train_mm).argmax(axis = 1) \n",
    "y_pred_nn_val=nn.predict(X_val_mm).argmax(axis = 1) \n",
    "print('minmax')\n",
    "print(f'acc on train set: {accuracy_score(y_train,y_pred_nn_train):.5f}, acc on validation set:{accuracy_score(y_val,y_pred_nn_val):.5f}')\n",
    "\n",
    "\n",
    "#expected output:\n",
    "#minmax\n",
    "#acc on train set: 0.94765, acc on validation set:0.89083\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7vBSsFVTQAM",
    "tags": []
   },
   "source": [
    "## What happen with Standard Scaler?\n",
    "\n",
    "Now I another ANN using the data preprocessed by the standard scaler. The accuracy score reached on the validation is a little better then the one reached before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "J8yNFvtnTVHc",
    "outputId": "e0e124c1-c74a-47d8-c922-04d647726fde"
   },
   "outputs": [],
   "source": [
    "#early stopping\n",
    "stop=EarlyStopping(monitor='val_loss', #quantity to be monitored\n",
    "                    mode='min', #we look for decreasing patterns stop \n",
    "                    patience = 15, #number of epochs with no improvements\n",
    "                    verbose = 1)\n",
    "\n",
    "#model defining\n",
    "np.random.seed(123)\n",
    "set_random_seed=2\n",
    "\n",
    "nns=Sequential()\n",
    "nns.add(Dense(input_dim=features,\n",
    "             units=features,\n",
    "             activation='relu'))\n",
    "nns.add(Dense(num_classes,\n",
    "             activation='softmax'))\n",
    "print(nns.summary())\n",
    "\n",
    "#compiling and training\n",
    "nns.compile(loss='categorical_crossentropy',\n",
    "           optimizer='sgd',\n",
    "           metrics=['accuracy'])\n",
    "train_s=nns.fit(X_train_ss,y_train_cat,\n",
    "             batch_size=16,\n",
    "             verbose=0,\n",
    "             callbacks=stop,\n",
    "             epochs=100,\n",
    "             validation_split=0.1)\n",
    "\n",
    "\n",
    "#display the accuracy \n",
    "plt.plot(train_s.history['accuracy'])\n",
    "plt.plot(train_s.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','validation split'],loc='down right',fontsize='x-large')\n",
    "plt.show()\n",
    "\n",
    "#make prediction on training and validation set\n",
    "y_pred_train_ss=nns.predict(X_train_ss).argmax(axis = 1) \n",
    "y_pred_val_ss=nns.predict(X_val_ss).argmax(axis = 1) \n",
    "print('Standard Scaler')\n",
    "print(f'acc on train set: {accuracy_score(y_train,y_pred_train_ss):.5f}, acc on validation set:{accuracy_score(y_val,y_pred_val_ss):.5f}')\n",
    "\n",
    "\n",
    "#expected output:\n",
    "#Standard Scaler\n",
    "#acc on train set: 0.97587, acc on validation set:0.89417"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from training import train_epochs_acc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset\n",
    "\n",
    "dataset = datasets.FashionMNIST('MNIST', train = True, download = True,\n",
    "                             transform = transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                                transforms.Normalize(0.0, 1.0, inplace=True),\n",
    "                                 transforms.RandomAffine(10, (0.1,0.1), (0.95,1)),\n",
    "                                 transforms.RandomCrop((28,28))\n",
    "                             ]))\n",
    "\n",
    "test_data = datasets.FashionMNIST('MNIST', train = False, download = True,\n",
    "                             transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                                 transforms.Normalize(0.0, 1.0, inplace=True)\n",
    "                             ]))\n",
    "\n",
    "# Divide data dataset in train dataset and val dataset\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataloaders\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_data, batch_size = batch_size, shuffle = True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_data, batch_size = batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some samples\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 4,4\n",
    "\n",
    "\n",
    "for i in range(1,cols * rows+1):\n",
    "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
    "    img, label = dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, activation, drop_p):\n",
    "        \"\"\"\n",
    "        The input is typically a MNIST images batch, encoded in a torch.tensor of size (N,1,28,28), where N is the batch size\n",
    "        -----------\n",
    "        Parameters:\n",
    "        act = activation function \n",
    "        drop_p = dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Parameters \n",
    "        self.act = getattr(nn, activation)(inplace = True)\n",
    "        self.drop_p = drop_p\n",
    "        \n",
    "        \n",
    "        ## Network architecture\n",
    "        # Convolution part\n",
    "        self.cnn = nn.Sequential(\n",
    "            #first convolution layer\n",
    "            nn.Conv2d(1, 16, 5),  # out = (N, 16, 24, 24)\n",
    "            nn.BatchNorm2d(16),\n",
    "            self.act,\n",
    "            nn.Dropout(self.drop_p, inplace = False),\n",
    "            nn.MaxPool2d(2),  # out = (N, 16, 12, 12)\n",
    "            # Second convolution layer\n",
    "            nn.Conv2d(16, 32, 5), # out = (N, 32, 8, 8)\n",
    "            nn.BatchNorm2d(32),\n",
    "            self.act,\n",
    "            nn.Dropout(self.drop_p, inplace = False),\n",
    "            nn.MaxPool2d(2) # out = (N, 32, 4, 4)\n",
    "        )\n",
    "    \n",
    "        # Linear classifier\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(in_features = 32*4*4, out_features = 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            self.act,\n",
    "            nn.Dropout(self.drop_p, inplace = False),\n",
    "            nn.Linear(in_features = 128, out_features = 10)\n",
    "        )\n",
    "\n",
    "        print(\"CNN initialized\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolution layer\n",
    "        x = self.cnn(x)\n",
    "        # Flatten layer\n",
    "        x = torch.flatten(x, start_dim = 1)\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Training device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizialize the network\n",
    "torch.manual_seed(0)\n",
    "net = ConvNet(activation=\"ReLU\", drop_p=0.3)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "# Training \n",
    "max_num_epochs = 1\n",
    "early_stopping = True\n",
    "train_loss_adam, val_loss_adam, accuracy_adam = train_epochs_acc(net, device, train_dataloader, val_dataloader, test_dataloader, loss_function, optimizer, max_num_epochs, early_stopping = early_stopping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot losses\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.semilogy(train_loss_adam, label='Train loss')\n",
    "ax1.semilogy(val_loss_adam, label='Validation loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.semilogy(accuracy_adam, label = \"Test accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Test accuracy (%)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "1u0DvQPgQ0Rh",
    "outputId": "4015c1be-7553-4a11-ac1a-f7c288de040c"
   },
   "outputs": [],
   "source": [
    "#k values tested\n",
    "k_values= [2,4,8,10,16,32] \n",
    "\n",
    "\n",
    "#minmax scaler data\n",
    "val_accuracy_mm = []\n",
    "\n",
    "for k in k_values:\n",
    "  model = KNeighborsClassifier(n_neighbors=k)\n",
    "  model.fit(X_train_mms, y_train_small)\n",
    "  y_pred_val = model.predict(X_val_mms)\n",
    "  val_accuracy_mm.append(accuracy_score(y_val_small,y_pred_val))\n",
    "\n",
    "\n",
    "\n",
    "#standard scaler data\n",
    "val_accuracy_ss=[]\n",
    "\n",
    "for k in k_values:\n",
    "  model = KNeighborsClassifier(n_neighbors=k)\n",
    "  model.fit(X_train_sss, y_train_small)\n",
    "  y_pred_val_sss = model.predict(X_val_sss)\n",
    "  val_accuracy_ss.append(accuracy_score(y_val_small,y_pred_val_sss))\n",
    "\n",
    "\n",
    "\n",
    "#hog trnasformed data\n",
    "val_accuracy_hog = []\n",
    "\n",
    "for k in k_values:\n",
    "  model = KNeighborsClassifier(n_neighbors=k)\n",
    "  model.fit(X_train_hog, y_train_small)\n",
    "  y_pred_val_hog = model.predict(X_val_hog)\n",
    "  val_accuracy_hog.append(accuracy_score(y_val_small,y_pred_val_hog))\n",
    "\n",
    "\n",
    "\n",
    "#accuracy on the small validation set\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(k_values, val_accuracy_mm, label=\"val acc minmax\")\n",
    "plt.plot(k_values, val_accuracy_ss, label=\"val acc StandScal\")\n",
    "plt.plot(k_values, val_accuracy_hog, label=\"val acc Hog\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Accuracy\")  \n",
    "plt.title('Accuracy score plot')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "model_mm = KNeighborsClassifier(n_neighbors=4)\n",
    "model_mm.fit(X_train_mm, y_train)\n",
    "y_pred_train_mm = model_mm.predict(X_train_mm)\n",
    "y_pred_val_mm = model_mm.predict(X_val_mm)\n",
    "print('Minmax scaled data')\n",
    "print(f\"accuracy score on train:{accuracy_score(y_train,y_pred_train_mm):.5}, on validation:{accuracy_score(y_val,y_pred_val_mm):.5f}\")\n",
    "\n",
    "model_ss = KNeighborsClassifier(n_neighbors=4)\n",
    "model_ss.fit(X_train_ss, y_train)\n",
    "y_pred_train_ss = model_ss.predict(X_train_ss)\n",
    "y_pred_val_ss = model_ss.predict(X_val_ss)\n",
    "print('Standard scaled data')\n",
    "print(f\"accuracy score on train:{accuracy_score(y_train,y_pred_train_ss):.5}, on validation:{accuracy_score(y_val,y_pred_val_ss):.5f}\")\n",
    "\n",
    "model_hog = KNeighborsClassifier(n_neighbors=4)\n",
    "model_hog.fit(X_train_hog, y_train_small)\n",
    "y_pred_train_hog = model_hog.predict(X_train_hog)\n",
    "y_pred_val_hog = model_hog.predict(X_val_hog)\n",
    "print('HOG transformed data')\n",
    "print(f\"accuracy score on train:{accuracy_score(y_train_small,y_pred_train_hog):.5}, on validation:{accuracy_score(y_val_small,y_pred_val_hog):.5f}\")\n",
    "\n",
    "\n",
    "#expected output:\n",
    "#Minmax scaled data\n",
    "#accuracy score on train:0.907, on validation:0.85833\n",
    "#Standard scaled data\n",
    "#accuracy score on train:0.9093, on validation:0.85750\n",
    "#HOG transformed data\n",
    "#accuracy score on train:0.8813, on validation:0.81667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7allnjL4BDI",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "I can finally watch the performance on the test set using the best model found: the artificial neural network (wich works a little better then the SVM). \n",
    "Before this I only need to preprocess the Test Set using MinMax Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "aXDMtucQS62C",
    "outputId": "ec2e6eeb-1ddf-4c86-db95-12a8c6ad5076"
   },
   "outputs": [],
   "source": [
    "#run time of final cell: 10minutes more or less\n",
    "\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import tensorflow \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Input, Dense \n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.python.framework.random_seed import set_random_seed\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#preprocessing\n",
    "X_train_val_mm=X_train_val/255\n",
    "X_test_mm=X_test/255\n",
    "y_train_val_cat=to_categorical(Y_train_val,10)\n",
    "\n",
    "#early stopping\n",
    "stop=EarlyStopping(monitor='val_loss', #quantity to be monitored\n",
    "                    mode='min', #we look for decreasing patterns stop \n",
    "                    patience = 15, #number of epochs with no improvements\n",
    "                    verbose = 1)\n",
    "\n",
    "#model defining\n",
    "np.random.seed(123)\n",
    "set_random_seed=2\n",
    "\n",
    "nn_final=Sequential()\n",
    "nn_final.add(Dense(input_dim=features,\n",
    "             units=features,\n",
    "             activation='relu'))\n",
    "nn_final.add(Dense(num_classes,\n",
    "             activation='softmax'))\n",
    "print(nn_final.summary())\n",
    "\n",
    "#compiling and training\n",
    "nn_final.compile(loss='categorical_crossentropy',\n",
    "           optimizer='sgd',\n",
    "           metrics=['accuracy'])\n",
    "train_final=nn_final.fit(X_train_val_mm,y_train_val_cat,\n",
    "             batch_size=16,\n",
    "             verbose=0,\n",
    "             callbacks=stop,\n",
    "             epochs=100,\n",
    "             validation_split=0.1)\n",
    "\n",
    "\n",
    "#display the accuracy and the loss\n",
    "fig = plt.figure(figsize=(7,4))\n",
    "plt.plot(train_final.history['accuracy'])\n",
    "plt.plot(train_final.history['val_accuracy'])\n",
    "plt.plot(train_final.history['loss'])\n",
    "plt.plot(train_final.history['val_loss'])\n",
    "plt.title('model accuracy & loss')\n",
    "plt.ylabel('accuracy / loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train accuracy','val accuracy','train loss','val loss'],loc='down right',fontsize='x-large')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#make prediction on training and validation set\n",
    "y_pred_test_final=nn_final.predict(X_test_mm).argmax(axis = 1) \n",
    "print('Accuracy')\n",
    "print(f'test set: {accuracy_score(Y_test,y_pred_test_final):.5f}')\n",
    "\n",
    "\n",
    "#expected output:\n",
    "#Accuracy\n",
    "#test set: 0.89100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgzkJkKRJwrK"
   },
   "source": [
    "The total run time of the notebook is 1 hour and 30 minutes"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VP4IOS4o4Ogg",
    "qyYUI8Th3f5b",
    "TehSSDy1Jbsx",
    "o6DExZfTLAE9",
    "Xm93nI0cRite",
    "4WBI374lu333",
    "l-5q5BrafMxw",
    "2Dnzhz-uyto7",
    "h7vBSsFVTQAM",
    "eBt3VunKB0Gc",
    "VWNcc7NCQk3t",
    "a7allnjL4BDI"
   ],
   "name": "Project_Fashion_Mnist_Lattaruolo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
